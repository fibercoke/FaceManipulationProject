{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from yolov3_tf2.utils import freeze_all\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ReduceLROnPlateau,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TensorBoard\n",
    ")\n",
    "import yolov3_tf2.dataset_chars as dataset\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE=13163\n",
    "VAL_SIZE=int(DATA_SIZE*0.15)\n",
    "\n",
    "flags.DEFINE_integer('size', 32, 'size of each character should be resize to')\n",
    "flags.DEFINE_integer('epochs', 50, 'epoch num')\n",
    "flags.DEFINE_integer('batch_size', 32, 'size of each batch')\n",
    "flags.DEFINE_string('dataset_path', './data/chars_data.tfrecord', 'path to output dataset file')\n",
    "flags.DEFINE_string('classes_path', './data/chars_data.names', 'path to output class file')\n",
    "\n",
    "app._run_init(['resnet'], app.parse_flags_with_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_2 (Model)              (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 34)                17442     \n",
      "=================================================================\n",
      "Total params: 24,916,898\n",
      "Trainable params: 1,329,186\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "all_dataset = dataset.load_tfrecord_dataset(FLAGS.dataset_path, size=FLAGS.size)\n",
    "all_dataset = all_dataset.map(lambda x, y: (dataset.transform_images(x, FLAGS.size), y))\n",
    "all_dataset = all_dataset.shuffle(buffer_size=1024)\n",
    "val_dataset, train_dataset = all_dataset.take(VAL_SIZE), all_dataset.skip(VAL_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).batch(FLAGS.batch_size)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).batch(FLAGS.batch_size)\n",
    "\n",
    "class_map = {name: idx for idx, name in enumerate(open(FLAGS.classes_path).read().splitlines())}\n",
    "class_num = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restnet = tf.keras.applications.ResNet50(classes=class_num, include_top=False, weights='imagenet',\n",
    "                                       input_shape=(FLAGS.size, FLAGS.size, 3))\n",
    "\n",
    "output = Flatten()(restnet.layers[-1].output)\n",
    "restnet = Model(inputs=restnet.input, outputs=output)\n",
    "for layer in restnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(restnet)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(class_num, activation='sigmoid'))\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ModelCheckpoint('chars_checkpoints/resnet50_train_{epoch}.tf', verbose=1,\n",
    "                    save_best_only=True, save_weights_only=True, mode='min'),\n",
    "    TensorBoard(log_dir='logs/chars/resnet50')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "    350/Unknown - 7s 20ms/step - loss: 1.5007 - accuracy: 0.5430\n",
      "Epoch 00001: val_loss improved from inf to 9.03939, saving model to chars_checkpoints/resnet50_train_1.tf\n",
      "350/350 [==============================] - 10s 28ms/step - loss: 1.5007 - accuracy: 0.5430 - val_loss: 9.0394 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.7927\n",
      "Epoch 00002: val_loss improved from 9.03939 to 5.25530, saving model to chars_checkpoints/resnet50_train_2.tf\n",
      "350/350 [==============================] - 8s 23ms/step - loss: 0.6880 - accuracy: 0.7931 - val_loss: 5.2553 - val_accuracy: 0.0334 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.8494\n",
      "Epoch 00003: val_loss improved from 5.25530 to 3.31825, saving model to chars_checkpoints/resnet50_train_3.tf\n",
      "350/350 [==============================] - 8s 23ms/step - loss: 0.4769 - accuracy: 0.8494 - val_loss: 3.3182 - val_accuracy: 0.0755 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8758\n",
      "Epoch 00004: val_loss improved from 3.31825 to 2.53371, saving model to chars_checkpoints/resnet50_train_4.tf\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.3708 - accuracy: 0.8757 - val_loss: 2.5337 - val_accuracy: 0.2021 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8943\n",
      "Epoch 00005: val_loss improved from 2.53371 to 2.30830, saving model to chars_checkpoints/resnet50_train_5.tf\n",
      "350/350 [==============================] - 8s 23ms/step - loss: 0.2975 - accuracy: 0.8943 - val_loss: 2.3083 - val_accuracy: 0.2857 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.9014\n",
      "Epoch 00006: val_loss improved from 2.30830 to 1.50837, saving model to chars_checkpoints/resnet50_train_6.tf\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.2823 - accuracy: 0.9014 - val_loss: 1.5084 - val_accuracy: 0.4103 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.9132\n",
      "Epoch 00007: val_loss did not improve from 1.50837\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.2499 - accuracy: 0.9134 - val_loss: 1.8501 - val_accuracy: 0.3906 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9230\n",
      "Epoch 00008: val_loss did not improve from 1.50837\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.2331 - accuracy: 0.9231 - val_loss: 2.0753 - val_accuracy: 0.3860 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "348/350 [============================>.] - ETA: 0s - loss: 0.2333 - accuracy: 0.9234\n",
      "Epoch 00009: val_loss improved from 1.50837 to 1.49041, saving model to chars_checkpoints/resnet50_train_9.tf\n",
      "350/350 [==============================] - 8s 23ms/step - loss: 0.2338 - accuracy: 0.9233 - val_loss: 1.4904 - val_accuracy: 0.5117 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.2139 - accuracy: 0.9250\n",
      "Epoch 00010: val_loss improved from 1.49041 to 1.35780, saving model to chars_checkpoints/resnet50_train_10.tf\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.2138 - accuracy: 0.9249 - val_loss: 1.3578 - val_accuracy: 0.5223 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9285\n",
      "Epoch 00011: val_loss did not improve from 1.35780\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.2074 - accuracy: 0.9284 - val_loss: 1.7164 - val_accuracy: 0.4534 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1979 - accuracy: 0.9318\n",
      "Epoch 00012: val_loss improved from 1.35780 to 0.98785, saving model to chars_checkpoints/resnet50_train_12.tf\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1976 - accuracy: 0.9319 - val_loss: 0.9879 - val_accuracy: 0.5618 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9402\n",
      "Epoch 00013: val_loss did not improve from 0.98785\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1825 - accuracy: 0.9401 - val_loss: 1.2771 - val_accuracy: 0.5339 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9344\n",
      "Epoch 00014: val_loss did not improve from 0.98785\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1987 - accuracy: 0.9344 - val_loss: 1.7463 - val_accuracy: 0.4498 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "348/350 [============================>.] - ETA: 0s - loss: 0.1895 - accuracy: 0.9348\n",
      "Epoch 00015: val_loss improved from 0.98785 to 0.97940, saving model to chars_checkpoints/resnet50_train_15.tf\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1905 - accuracy: 0.9344 - val_loss: 0.9794 - val_accuracy: 0.5426 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "348/350 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9397\n",
      "Epoch 00016: val_loss improved from 0.97940 to 0.78735, saving model to chars_checkpoints/resnet50_train_16.tf\n",
      "350/350 [==============================] - 8s 23ms/step - loss: 0.1776 - accuracy: 0.9398 - val_loss: 0.7874 - val_accuracy: 0.6363 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "348/350 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9486\n",
      "Epoch 00017: val_loss improved from 0.78735 to 0.75485, saving model to chars_checkpoints/resnet50_train_17.tf\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1523 - accuracy: 0.9484 - val_loss: 0.7548 - val_accuracy: 0.6221 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "348/350 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9454\n",
      "Epoch 00018: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1597 - accuracy: 0.9453 - val_loss: 1.1336 - val_accuracy: 0.5066 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9444\n",
      "Epoch 00019: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1723 - accuracy: 0.9445 - val_loss: 1.2449 - val_accuracy: 0.4883 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9468\n",
      "Epoch 00020: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1639 - accuracy: 0.9468 - val_loss: 0.8248 - val_accuracy: 0.6327 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "348/350 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9495\n",
      "Epoch 00021: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1527 - accuracy: 0.9497 - val_loss: 0.7937 - val_accuracy: 0.6474 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9536\n",
      "Epoch 00022: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1452 - accuracy: 0.9535 - val_loss: 0.7874 - val_accuracy: 0.6474 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9533\n",
      "Epoch 00023: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1410 - accuracy: 0.9533 - val_loss: 1.0437 - val_accuracy: 0.5765 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9568\n",
      "Epoch 00024: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1330 - accuracy: 0.9567 - val_loss: 0.9832 - val_accuracy: 0.5876 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9571\n",
      "Epoch 00025: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 7s 21ms/step - loss: 0.1315 - accuracy: 0.9571 - val_loss: 0.8910 - val_accuracy: 0.5937 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.9539\n",
      "Epoch 00026: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1403 - accuracy: 0.9540 - val_loss: 1.0917 - val_accuracy: 0.5410 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.9547\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.75485\n",
      "350/350 [==============================] - 8s 22ms/step - loss: 0.1378 - accuracy: 0.9547 - val_loss: 1.3091 - val_accuracy: 0.4473 - lr: 0.0010\n",
      "Epoch 00027: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=FLAGS.epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = dataset.load_tfrecord_dataset(FLAGS.dataset_path, size=FLAGS.size)\n",
    "all_dataset = all_dataset.map(lambda x, y: (tf.expand_dims(dataset.transform_images(x, FLAGS.size), axis=0), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TFRecordDataset('./data/chars_data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0507 15:06:41.575154 140152253048640 image.py:708] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f761c03f520>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(label.numpy())\n",
    "plt.imshow(img[0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
